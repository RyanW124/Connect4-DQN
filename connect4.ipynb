{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:22.683428Z",
     "start_time": "2023-09-01T19:57:22.665858800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import ModuleList, functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "class Board:\n",
    "    dx = [1, 0, 1, 1]\n",
    "    dy = [0, 1, 1, -1]\n",
    "    def __init__(self) -> None:\n",
    "        self.board = [[0]*6 for _ in range(7)]\n",
    "        self.turn = 1\n",
    "        self.played = set()\n",
    "        self.win = None\n",
    "        self.columns = [0] * 7\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "        for i in reversed(list(zip(*self.board))):\n",
    "            ret += \"|\"\n",
    "            for j in i:\n",
    "                ret += str(j).rjust(3)\n",
    "            ret += \" |\\n\"\n",
    "        return ret\n",
    "    def play(self, col):\n",
    "        if self.columns[col] >= 6: return\n",
    "        self.board[col][self.columns[col]] = self.turn\n",
    "        self.played.add((col, self.columns[col]))\n",
    "        self.columns[col] += 1\n",
    "        self.turn *= -1\n",
    "        self.win = self.check_win()\n",
    "        return self.win\n",
    "    def reward(self, col=-1, *, inverted=1):\n",
    "        total = 0\n",
    "        if col != -1:\n",
    "            self.board[col][self.columns[col]] = self.turn\n",
    "        for posx in range(7):\n",
    "            for posy in range(6):\n",
    "                color = self.board[posx][posy]\n",
    "                if color == 0:\n",
    "                    continue\n",
    "                for k in range(4):\n",
    "                    count = 1\n",
    "                    multiplier = 2\n",
    "                    for sign in [-1, 1]:\n",
    "                        for l in range(1, 4): \n",
    "                            r = posx + self.dx[k]*l*sign\n",
    "                            c = posy + self.dy[k]*l*sign\n",
    "                            if not (0 <= r < 7 and 0 <= c < 6):\n",
    "                                multiplier -= 1\n",
    "                                break\n",
    "                            if self.board[r][c] !=color:\n",
    "                                if self.board[r][c] == -color: multiplier -= 1\n",
    "                                break\n",
    "                            # print(r, c)\n",
    "                            count += 1\n",
    "                            if count >= 4:\n",
    "                                if col != -1: self.board[col][self.columns[col]] = 0\n",
    "                                return 20\n",
    "                    # print('mc',multiplier, count)\n",
    "                    total += (multiplier * count) **3/30 * (1 if color == self.turn*inverted else -1)\n",
    "        if col != -1:\n",
    "            self.board[col][self.columns[col]] = 0\n",
    "        \n",
    "        return total\n",
    "    def reward_of(self, col):\n",
    "        posx, posy = pos = (col, self.columns[col])\n",
    "        color = self.turn\n",
    "        total = 0\n",
    "        for k in range(4):\n",
    "            count = 1\n",
    "            multiplier = 2\n",
    "            for sign in [-1, 1]:\n",
    "                for l in range(1, 4): \n",
    "                    r = posx + self.dx[k]*l*sign\n",
    "                    c = posy + self.dy[k]*l*sign\n",
    "                    if not (0 <= r < 7 and 0 <= c < 6):\n",
    "                        multiplier -= 1\n",
    "                        break\n",
    "                    if self.board[r][c] !=color:\n",
    "                        if self.board[r][c] == -color: multiplier -= 1\n",
    "                        break\n",
    "                    # print(r, c)\n",
    "                    count += 1\n",
    "                    if count >= 4:\n",
    "                        return 20\n",
    "            # print('mc',multiplier, count)\n",
    "            total += (multiplier * count) **3/30\n",
    "        return total\n",
    "    def inverted(self):\n",
    "        return [[j*(-1) for j in i] for i in self.board]\n",
    "    def check_win(self):\n",
    "        for i, j in self.played:\n",
    "            color = self.board[i][j]\n",
    "            for k in range(3):\n",
    "                for l in range(1, 4):\n",
    "                    r = i + self.dx[k]*l \n",
    "                    c = j + self.dy[k]*l \n",
    "                    if not (0 <= r < 7 and 0 <= c < 6): break\n",
    "                    if self.board[r][c] != color: break\n",
    "                else:\n",
    "                    return color\n",
    "        if sum(self.columns) == 42: return 0\n",
    "        return None\n",
    "    def to_tensor(self, turn=None, *, make_batch=False):\n",
    "        if make_batch:\n",
    "            return torch.tensor([[self.board if turn == None and self.turn == 1 else self.inverted()]], dtype=torch.float32)\n",
    "\n",
    "        return torch.tensor([self.board if turn == None and self.turn == 1 else self.inverted()], dtype=torch.float32)\n",
    "    def legal(self):\n",
    "        return {i for i in range(7) if self.columns[i] < 6}\n",
    "    def filter_illegal_max(self, vals):\n",
    "        m = None\n",
    "        for i in self.legal():\n",
    "            if m is None or vals[0][m]< vals[0][i]:\n",
    "                m = i\n",
    "        return m\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (6x4 and 24x120)\n",
    "        # self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.fc0 = nn.Linear(42, 80)\n",
    "        self.fc1 = nn.Linear(80, 60)\n",
    "        self.fc2 = nn.Linear(60, 20)\n",
    "        self.fc3 = nn.Linear(20, 7)\n",
    "        for i in [self.fc0, self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.kaiming_normal_(i.weight)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.00025)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "WIN = 10\n",
    "BATCH_SIZE = 10\n",
    "GAMMA = 0.9\n",
    "class Agent:\n",
    "    def __init__(self, board: Board) -> None:\n",
    "        self.model = NN()\n",
    "        self.t_model = NN()\n",
    "        self.update_t = 0\n",
    "        self.exp = deque()\n",
    "        self.interval = 50\n",
    "        self.losses_x = []\n",
    "        self.losses_y = []\n",
    "        self.x = 0\n",
    "        self.epsilon = 0\n",
    "        self.board = board\n",
    "        self.loss_f = nn.MSELoss()\n",
    "        self.copy()\n",
    "    def copy(self):\n",
    "        self.t_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def legal_mask(self, states):\n",
    "        ret = []\n",
    "        for board in states:\n",
    "            row = [1 if 0 in col else 0 for col in board]\n",
    "            ret.append(row)\n",
    "        return torch.tensor(ret, dtype=torch.float32)\n",
    "                \n",
    "        \n",
    "    def step(self):\n",
    "        s = self.board.to_tensor()\n",
    "        if random.random() < self.epsilon:\n",
    "            a = self.board.filter_illegal_max(self.model(self.board.to_tensor(make_batch=True)))\n",
    "        else:\n",
    "            a = random.choice(list(self.board.legal()))\n",
    "        r = self.board.reward()\n",
    "        self.board.play(a)\n",
    "        if self.board.win is None:\n",
    "            new_board = deepcopy(self.board)\n",
    "            oppo_a = new_board.filter_illegal_max(self.model(new_board.to_tensor(make_batch=True)))\n",
    "            # print(oppo_a)\n",
    "            # r -= new_board.reward_of(oppo_a) * 1.1\n",
    "            new_board.play(oppo_a)\n",
    "            r = new_board.reward() - r\n",
    "            new_s = new_board.to_tensor()\n",
    "            # print(a, oppo_a)\n",
    "        else:\n",
    "            new_s = self.board.to_tensor(-1)\n",
    "            r = 0 if board.win == 0 else 10000\n",
    "        # print(\"new\", r)\n",
    "        # print()\n",
    "        \n",
    "        # print(r)\n",
    "        # print()\n",
    "        # new_s = self.board.to_tensor(-1)\n",
    "        self.exp.append((s, a, r, new_s))\n",
    "        if len(self.exp) < BATCH_SIZE:\n",
    "            return\n",
    "        self.model.optimizer.zero_grad()\n",
    "        batch = random.sample(self.exp, BATCH_SIZE)\n",
    "        states, actions, rewards, new_states = zip(*batch)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        # print(actions)\n",
    "        for _ in range(1):\n",
    "            q = self.model(torch.stack(states)).gather(1, torch.tensor(actions, dtype=torch.int64).unsqueeze(-1)).flatten(0)\n",
    "            target_q = (self.t_model(torch.stack(new_states))*self.legal_mask(new_states)).max(1)[0]*GAMMA + rewards\n",
    "            # for i in states: print(i)\n",
    "            # print(q)\n",
    "            # print(rewards)\n",
    "            # print(target_q)\n",
    "            loss = self.loss_f(q, target_q)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.model.optimizer.step()\n",
    "        self.update_t += 1\n",
    "        # if self.update_t % 5 == 0:\n",
    "        #         self.losses_y.append(int(loss))\n",
    "        #         self.losses_x.append(self.x)\n",
    "        #         self.x+=1\n",
    "        if self.update_t >= self.interval:\n",
    "            self.update_t = 0\n",
    "            self.copy()\n",
    "        if self.board.win is not None:\n",
    "            # print(self.board)\n",
    "            self.epsilon += (1-self.epsilon)* 0.05\n",
    "            self.board = Board()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:22.801543500Z",
     "start_time": "2023-09-01T19:57:22.780979500Z"
    }
   },
   "id": "87e336f03cac5835"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:11<00:00, 453.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "board = Board()\n",
    "p1 = Agent(board)\n",
    "for i in tqdm(range(5000)):\n",
    "    p1.step()\n",
    "plt.scatter(p1.losses_x, p1.losses_y)\n",
    "plt.ylim([0, 2e7])\n",
    "plt.show()\n",
    "# add legal mask to the target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:33.900804500Z",
     "start_time": "2023-09-01T19:57:22.802542400Z"
    }
   },
   "id": "2a6ebbd6205d8e71"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI1 played 4: tensor([[88800.9297, 81848.9766, 77522.2344, 46648.0742, 93856.6328, 83832.0078,\n",
      "         85191.0000]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.1, 0.36666666666666664, 0.36666666666666664, 0.36666666666666664, 0.36666666666666664, 0.36666666666666664, 0.1]\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "\n",
      "AI2 played 4: tensor([[71641.7266, 66033.6797, 62541.5508, 37632.6758, 75721.2891, 67629.1328,\n",
      "         68726.4297]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.09999999999999998, 0.36666666666666664, 0.36666666666666664, 0.36666666666666664, 0.8666666666666667, 0.36666666666666664, 0.09999999999999998]\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "\n",
      "AI1 played 4: tensor([[132077.2344, 121737.5547, 115304.2344,  69385.4141, 139595.6562,\n",
      "         124695.4922, 126712.7266]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.10000000000000003, 0.36666666666666664, 0.36666666666666664, 4.3, 0.8666666666666667, 4.299999999999999, 0.09999999999999998]\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "\n",
      "AI2 played 4: tensor([[69503.7734, 64063.8594, 60675.6211, 36508.8945, 73461.3359, 65608.1875,\n",
      "         66671.4297]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.09999999999999998, 0.36666666666666664, 0.36666666666666664, 0.6, 0.8666666666666667, 0.5999999999999999, 0.09999999999999998]\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "\n",
      "AI1 played 4: tensor([[191735.3594, 176721.8906, 167382.8438, 100722.8281, 202652.6875,\n",
      "         181014.8594, 183949.6094]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.10000000000000003, 0.36666666666666664, 0.36666666666666664, 4.3, 0.8666666666666667, 4.3, 0.09999999999999998]\n",
      "|  0  0  0  0  0  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "\n",
      "AI2 played 4: tensor([[105260.3984,  97022.2266,  91891.5000,  55291.4180, 111253.2188,\n",
      "          99361.0625, 100970.8047]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.09999999999999998, 0.36666666666666664, 0.36666666666666664, 0.6, 0.36666666666666664, 0.5999999999999999, 0.09999999999999998]\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "\n",
      "AI1 played 0: tensor([[253339.8906, 233501.4375, 221162.3281, 133083.5156, 267766.3438,\n",
      "         239172.5156, 243053.5156]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.10000000000000005, 0.36666666666666664, 0.36666666666666664, 4.3, 0, 4.3, 0.09999999999999999]\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "\n",
      "AI2 played 0: tensor([[136552.0312, 125865.9453, 119208.9688,  71728.9688, 144325.3125,\n",
      "         128900.4844, 130984.6562]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.16666666666666669, 0.16666666666666669, 0.36666666666666675, 0.6, 0, 0.6000000000000001, 0.09999999999999999]\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "\n",
      "AI1 played 0: tensor([[313494.3438, 288945.7812, 273676.3750, 164683.1562, 331346.7500,\n",
      "         295961.1250, 300767.0000]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.16666666666666669, 0.6000000000000001, 0.36666666666666664, 4.299999999999999, 0, 4.299999999999999, 0.1]\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "\n",
      "AI2 played 0: tensor([[163523.2656, 150727.0156, 142754.7344,  85896.9453, 172831.3750,\n",
      "         154360.8281, 156855.4062]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.16666666666666669, 0.10000000000000005, 0.36666666666666675, 0.6, 0, 0.6000000000000001, 0.09999999999999999]\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  0  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "\n",
      "AI1 played 0: tensor([[363985.1562, 335483.4688, 317753.3750, 191206.6094, 384713.5312,\n",
      "         343626.8750, 349208.0938]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.16666666666666669, 0.6000000000000001, 0.36666666666666664, 4.299999999999999, 0, 4.299999999999999, 0.1]\n",
      "|  0  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "\n",
      "AI2 played 0: tensor([[192446.2031, 177385.8125, 168003.6719, 101089.1172, 203401.3750,\n",
      "         181662.2656, 184599.6406]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0.10000000000000005, 0.10000000000000005, 0.36666666666666675, 0.6, 0, 0.6000000000000001, 0.09999999999999999]\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "\n",
      "AI1 played 6: tensor([[425715.3438, 392381.3438, 371643.9375, 223635.9062, 449958.1250,\n",
      "         401906.6250, 408432.3438]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000001, 0.36666666666666664, 4.3, 0, 4.3, 0.09999999999999999]\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "\n",
      "AI2 played 6: tensor([[221073.8281, 203773.5781, 192996.3125, 116127.4609, 233658.2500,\n",
      "         208687.9844, 212058.6094]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.09999999999999996, 0.36666666666666664, 0.6000000000000001, 0, 0.6, 0.16666666666666666]\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "\n",
      "AI1 played 6: tensor([[475100.4375, 437900.1250, 414756.9688, 249578.9375, 502154.5312,\n",
      "         448529.7188, 455812.7500]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000001, 0.3666666666666667, 4.3, 0, 2.7000000000000006, 0.16666666666666666]\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "\n",
      "AI2 played 6: tensor([[248748.7656, 229283.0156, 217156.3125, 130665.1172, 262908.2188,\n",
      "         234813.2344, 238604.4531]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.09999999999999996, 0.36666666666666664, 0.6000000000000001, 0, 0.5333333333333332, 0.16666666666666666]\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  0 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "\n",
      "AI1 played 6: tensor([[530749.3750, 489191.7188, 463337.5625, 278812.4688, 560972.3125,\n",
      "         501067.5000, 509202.8750]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000001, 0.3666666666666667, 4.3, 0, 2.7000000000000006, 0.16666666666666666]\n",
      "| -1  0  0  0 -1  0  0 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "\n",
      "AI2 played 6: tensor([[275071.1250, 253545.5625, 240135.6406, 144492.0781, 290729.3125,\n",
      "         259662.0000, 263852.9375]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.09999999999999996, 0.36666666666666664, 0.6000000000000001, 0, 0.5333333333333332, 0.09999999999999999]\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "\n",
      "AI1 played 5: tensor([[582870.6875, 537231.6875, 508839.2500, 306192.8438, 616062.6250,\n",
      "         550274.5625, 559209.1250]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000001, 0.36666666666666664, 4.3, 0, 2.7, 0]\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "\n",
      "AI2 played 5: tensor([[303224.7500, 279495.7500, 264714.0938, 159281.4844, 320485.5000,\n",
      "         286239.1875, 290859.0625]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.09999999999999964, 0.36666666666666625, 3.066666666666667, 0, 2.8000000000000003, 0]\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "\n",
      "AI1 played 5: tensor([[625318.8750, 576355.5625, 545895.1250, 328490.4688, 660928.6875,\n",
      "         590348.9375, 599934.3125]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000002, 0.3666666666666668, 20.1, 0, 3.0, 0]\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "\n",
      "AI2 played 5: tensor([[327948.4375, 302285.0625, 286297.9062, 172269.1875, 346616.1875,\n",
      "         309579.6250, 314572.4375]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.09999999999999964, 0.36666666666666625, 2.7666666666666666, 0, 3.0, 0]\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  0  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "\n",
      "AI1 played 5: tensor([[677794.2500, 624722.6875, 591706.5000, 356057.5000, 716391.3750,\n",
      "         639891.4375, 650278.2500]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000002, 0.3666666666666668, 20.1, 0, 2.8000000000000003, 0]\n",
      "| -1  0  0  0 -1  0 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "\n",
      "AI2 played 5: tensor([[356887.0625, 328958.6562, 311562.0625, 187470.8125, 377201.8125,\n",
      "         336898.4062, 342329.9688]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.09999999999999964, 0.3666666666666667, 2.7666666666666666, 0, 2.7, 0]\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "\n",
      "AI1 played 1: tensor([[729163.8125, 672070.1875, 636552.2500, 383042.7500, 770685.6250,\n",
      "         688388.3125, 699560.8125]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000004, 0.36666666666666703, 20.0, 0, 0, 0]\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "\n",
      "AI2 played 1: tensor([[384109.6562, 354050.8438, 335326.8125, 201770.7812, 405973.4375,\n",
      "         362595.9062, 368441.3750]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.7000000000000004, 0.6666666666666671, 2.766666666666667, 0, 0, 0]\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "\n",
      "AI1 played 1: tensor([[759495.2500, 700025.8750, 663031.1250, 398975.7812, 802743.5000,\n",
      "         717022.8750, 728659.0000]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.7000000000000004, 2.2666666666666666, 20.1, 0, 0, 0]\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "\n",
      "AI2 played 1: tensor([[411695.5625, 379478.1250, 359409.5312, 216261.8750, 435128.6250,\n",
      "         388638.0312, 394900.7500]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.7000000000000004, 0.6000000000000004, 2.766666666666667, 0, 0, 0]\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  0  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "\n",
      "AI1 played 1: tensor([[791130.6250, 729185.1875, 690649.8750, 415594.4688, 836178.8750,\n",
      "         746890.3125, 759009.1250]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.7000000000000004, 2.2666666666666666, 20.1, 0, 0, 0]\n",
      "| -1  0  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "\n",
      "AI2 played 1: tensor([[433565.3125, 399636.9688, 378502.8125, 227750.5312, 458242.0625,\n",
      "         409284.1875, 415877.4688]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0.6000000000000004, 0.6000000000000004, 2.766666666666667, 0, 0, 0]\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "\n",
      "AI1 played 2: tensor([[814141.0000, 750393.5625, 710737.8750, 427681.2812, 860498.8750,\n",
      "         768613.7500, 781084.7500]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 2.2666666666666666, 20.0, 0, 0, 0]\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "\n",
      "AI2 played 2: tensor([[450329.0625, 415088.5938, 393137.3438, 236555.7656, 475959.4688,\n",
      "         425108.3125, 431958.3438]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 2.3666666666666667, 5.433333333333334, 0, 0, 0]\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "\n",
      "AI1 played 2: tensor([[811566.0000, 748020.3750, 708491.0000, 426328.9688, 857777.7500,\n",
      "         766183.5625, 778614.5000]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 2.3666666666666663, 20.1, 0, 0, 0]\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "\n",
      "AI2 played 2: tensor([[458602.3750, 422714.7500, 400360.5938, 240902.0312, 484703.4375,\n",
      "         432918.5312, 439894.0000]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 2.3666666666666663, 5.366666666666667, 0, 0, 0]\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  0  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "\n",
      "AI1 played 2: tensor([[817348.1875, 753350.1250, 713538.1250, 429366.2188, 863888.8750,\n",
      "         771642.8750, 784160.7500]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 2.366666666666666, 20.1, 0, 0, 0]\n",
      "| -1 -1  0  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "\n",
      "AI2 played 2: tensor([[458000.5000, 422159.9688, 399835.2500, 240586.0625, 484067.7500,\n",
      "         432350.5312, 439316.1250]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 2.2666666666666657, 5.366666666666666, 0, 0, 0]\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "\n",
      "AI1 played 3: tensor([[812802.0625, 749160.6875, 709569.8750, 426978.5938, 859083.8750,\n",
      "         767351.1250, 779799.0000]], grad_fn=<AddmmBackward0>)\n",
      "reward: [0, 0, 0, 20.0, 0, 0, 0]\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  0  1  1  1 |\n",
      "| -1 -1 -1  0 -1 -1 -1 |\n",
      "|  1  1  1  1  1  1  1 |\n",
      "\n",
      "AI1 Won\n"
     ]
    }
   ],
   "source": [
    "board = Board()\n",
    "p1.board = board\n",
    "while board.win is None:\n",
    "    # if board.turn == 1:\n",
    "    #     a = int(input(\"Move: \"))\n",
    "    #     board.play(a)\n",
    "    # else:\n",
    "    values = p1.model(board.to_tensor(make_batch=True))\n",
    "    # print(f\"reward: {[board.reward_of(i) if i in board.legal() else 0 for i in range(7)]}\")\n",
    "    a = board.filter_illegal_max(values)\n",
    "    r = board.reward()\n",
    "    print(f\"AI{1 if board.turn == 1 else 2} played {a}: {values}\")\n",
    "    # print(f\"reward: {[board.reward_of(i) if i in board.legal() else 0 for i in range(7)]}\")\n",
    "    print(f\"reward: {[board.reward(i) - r if i in board.legal() else 0 for i in range(7)]}\")\n",
    "    board.play(a)\n",
    "    print(board)\n",
    "print(f\"AI{1 if board.win == 1 else 2} Won\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:33.981635Z",
     "start_time": "2023-09-01T19:57:33.896143100Z"
    }
   },
   "id": "2f74d37998af0c7b"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36666666666666664\n"
     ]
    }
   ],
   "source": [
    "print(Board().reward_of(3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:33.981635Z",
     "start_time": "2023-09-01T19:57:33.920744900Z"
    }
   },
   "id": "58c612f78cbc8c16"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "board = Board()\n",
    "p2 = Agent(board)\n",
    "for i in range(1000):\n",
    "    p2.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:36.078610700Z",
     "start_time": "2023-09-01T19:57:33.925254300Z"
    }
   },
   "id": "2e5afbde46a0f054"
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([34.2021, 18.4025, 14.8666, 51.9455, 49.9733, 23.4268, 29.2651, 40.2051,\n",
      "        33.6575, 59.8000], grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([-0.1000, -4.3000,  0.9667, -0.3333, -1.5000,  0.0000, -3.0667,  0.1000,\n",
      "        -0.4000, -0.6000])\n",
      "tensor([-5.6794, 32.9013, 32.0893, -6.6828, 17.9200,  6.6775, 18.5650,  3.1218,\n",
      "        11.1597, -2.3496], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = random.sample(p2.exp, BATCH_SIZE)\n",
    "states, actions, rewards, new_states = zip(*batch)\n",
    "rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "# print(actions)\n",
    "q = p2.model(torch.stack(states)).gather(1, torch.tensor(actions, dtype=torch.int64).unsqueeze(-1))\n",
    "\n",
    "print(q.flatten(0))\n",
    "target_q = (p2.t_model(torch.stack(new_states))*p2.legal_mask(new_states)).max(1)[0]*GAMMA + rewards\n",
    "print(rewards)\n",
    "print(target_q-q.flatten(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:36.089913800Z",
     "start_time": "2023-09-01T19:57:36.078610700Z"
    }
   },
   "id": "41dad9aa31620b15"
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6953, 0.8502, 0.2077, 0.9421],\n",
      "        [0.1880, 0.1321, 0.9138, 0.4057],\n",
      "        [0.0142, 0.1131, 0.7706, 0.4155]])\n",
      "tensor([[0.2077],\n",
      "        [0.4057],\n",
      "        [0.1131]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((3, 4))\n",
    "print(x)\n",
    "print(x.gather(1, torch.tensor([2, 3, 1]).unsqueeze(-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:36.101386900Z",
     "start_time": "2023-09-01T19:57:36.084412800Z"
    }
   },
   "id": "135e95f3acffbbc8"
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T19:57:36.101386900Z",
     "start_time": "2023-09-01T19:57:36.092476700Z"
    }
   },
   "id": "2bb1f1f85bfa38ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
